{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 0. Environment Setting"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10371,"status":"ok","timestamp":1715993718172,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"-hgTIv2duzmW"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import random\n","import time\n","import math\n","import re\n","import numpy as np\n","import copy\n","import os\n","\n","# Enable CUDA debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","# Disable cuDNN to isolate if the issue is with cuDNN\n","torch.backends.cudnn.enabled = False\n","torch.backends.cudnn.benchmark = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18338,"status":"ok","timestamp":1715993748143,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"OginIo4yy4rM","outputId":"b1d3dac1-4bfd-4c77-9261-1e0cccfb9048"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["TRAIN_PATH = 'Cooking_Dataset/train.csv'\n","DEV_PATH = 'Cooking_Dataset/dev.csv'\n","TEST_PATH = 'Cooking_Dataset/test.csv'\n","GLOVE_PATH = 'glove.6B.50d.txt'"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Data Preparation"]},{"cell_type":"markdown","metadata":{},"source":["## 1.1 Read data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7934,"status":"ok","timestamp":1715993756075,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"CDIx2dGszcmq","outputId":"063cb690-1e50-4f60-a48e-cd7f24e5cedf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test DataFrame:\n","                                             Ingredients  \\\n","9450   1 ea egg beaten\\t13 c  sugar\\t14 c  vinegar\\t1...   \n","34585  9 oz elbow macaroni\\t34 lb mild or hot italian...   \n","59368  1 12 c  uncle bens converted\\trice\\t12 c  wate...   \n","28309  1    whole red snapper 2 lb or whole sea bass ...   \n","36372  13 c  olive oil\\t2 tb cider vinegar\\t1    gree...   \n","\n","                                                  Recipe  \n","9450   in small saucepan  beat egg and sugar until we...  \n","34585  cook the macaroni in water according to direct...  \n","59368  preheat oven to 350 degrees  combine ingredien...  \n","28309  season fish inside and out with salt and peppe...  \n","36372  whisk together the oil  vinegar  scallion  mus...  \n"]}],"source":["# Basic data loading and processing\n","def clean_text(df, columns):\n","    for col in columns:\n","        df[col] = df[col].fillna('unknown')\n","        df[col] = df[col].astype(str)\n","        df[col] = df[col].apply(lambda x: re.sub(r'\\t', ' ', x))\n","        df[col] = df[col].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s]', '', x).lower().strip())\n","    return df\n","\n","# Load data\n","train_data = pd.read_csv(TRAIN_PATH)\n","dev_data = pd.read_csv(DEV_PATH)\n","test_data = pd.read_csv(TEST_PATH)\n","\n","# Clean data\n","columns = ['Ingredients', 'Recipe']\n","train_data = clean_text(train_data, columns)\n","dev_data = clean_text(dev_data, columns)\n","test_data = clean_text(test_data, columns)\n","\n","#Sampling\n","train_data_sampling = train_data.sample(frac=0.5, random_state=33)"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Word2idx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1715993756557,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"TvUsaefWFwq8","outputId":"df59602e-a3f3-47e4-e59c-5031e24d730f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ini_vocab size: 4917\n","{'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3, 'arsley': 4, 'freeform': 5, 'cherry': 6, 'opptional': 7, 'pref': 8, 'blended': 9}\n"]}],"source":["# Basic Word2idx Vocab\n","def build_vocab(data):\n","    words = set()\n","    for text in pd.concat([data['Ingredients'], data['Recipe']]):\n","        words.update(text.split())\n","    word2idx = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n","    for i, word in enumerate(words, 4):\n","        word2idx[word] = i\n","    return word2idx\n","\n","# setting vocab and vocab size\n","vocab  = build_vocab(train_data_sampling)\n","\n","# Inverse dictionary\n","idx2word = {idx: word for word, idx in vocab.items()}\n","\n","print(\"vocab size:\", len(vocab))\n","print({k: vocab[k] for k in list(vocab)[:20]})"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 Recipe "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715993756557,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"-_U9zdmyyrmD"},"outputs":[],"source":["MAX_LENGTH = 150\n","BATCH_SIZE = 32\n","\n","# Setting the recipe\n","class RecipeDataset(Dataset):\n","    def __init__(self, ingredients, recipes, vocab, max_length=MAX_LENGTH):\n","        self.ingredients = [self.encode(ing, vocab, max_length) for ing in ingredients]\n","        self.recipes = [self.encode(rec, vocab, max_length) for rec in recipes]\n","        self.vocab = vocab\n","\n","    def encode(self, text, vocab, max_length):\n","        encoded = [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in text.split()] + [vocab['<eos>']]\n","        if len(encoded) < max_length:\n","            encoded += [vocab['<pad>']] * (max_length - len(encoded))\n","        else:\n","            encoded = encoded[:max_length]\n","        return encoded\n","\n","    def __len__(self):\n","        return len(self.ingredients)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.ingredients[idx]), torch.tensor(self.recipes[idx])\n","\n","\n","# Create dataset and loader\n","train_dataset = RecipeDataset(train_data_sampling['Ingredients'].tolist(), train_data_sampling['Recipe'].tolist(), vocab)\n","dev_dataset = RecipeDataset(dev_data['Ingredients'].tolist(), dev_data['Recipe'].tolist(), vocab)\n","test_dataset = RecipeDataset(test_data['Ingredients'].tolist(), test_data['Recipe'].tolist(), vocab)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Baseline"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1.  Baseline 1: Sequence-to-Sequence model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":273,"status":"ok","timestamp":1715997025147,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"P0Ix2bgj07E1"},"outputs":[],"source":["NUM_LAYER = 2\n","\n","# Basic Encoder\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n","        super().__init__()\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=NUM_LAYER, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        embedded = self.dropout(self.embedding(src))\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        return outputs, hidden, cell\n","\n","# Basic Encoder and Decoder\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n","        super().__init__()\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.output_dim = output_dim\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=NUM_LAYER, batch_first=True)\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, cell):\n","        input = input.unsqueeze(1)\n","        embedded = self.dropout(self.embedding(input))\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        prediction = self.fc_out(output.squeeze(1))\n","        return prediction, hidden, cell"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1715997027948,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"MeJHoIPd077I"},"outputs":[],"source":["TEACHING_FORCING_RATIO = 0.5\n","\n","# Basic SEQ2SEQ\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=TEACHING_FORCING_RATIO):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","\n","        # Obtain encoder outputs\n","        encoder_outputs, hidden, cell = self.encoder(src)\n","\n","        input = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            outputs[:, t, :] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["## 2.2. Baseline 2: Sequence-to-Sequence model with Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":326,"status":"ok","timestamp":1715997035875,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"4blRRv8c1AUw"},"outputs":[],"source":["# Attention Mechanism\n","class Attention(nn.Module):\n","    def __init__(self, hid_dim):\n","        super().__init__()\n","        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n","        self.v = nn.Parameter(torch.rand(hid_dim))\n","\n","    def forward(self, hidden, encoder_outputs):\n","        # hidden: [batch_size, hid_dim]\n","        # encoder_outputs: [batch_size, src_len, hid_dim]\n","        src_len = encoder_outputs.shape[1]\n","\n","        # Repeat hidden state src_len times\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hid_dim]\n","        attention = torch.sum(self.v * energy, dim=2)  # [batch_size, src_len]\n","\n","        return torch.softmax(attention, dim=1)  # [batch_size, src_len]\n","\n","\n","# Attention Decoder\n","class DecoderWithAttention(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, attention, dropout):\n","        super().__init__()\n","        # important\n","        self.output_dim = output_dim\n","        self.attention = attention\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.rnn = nn.LSTM(hid_dim + emb_dim, hid_dim, num_layers=NUM_LAYER, batch_first=True)\n","        # self.fc_out = nn.Linear(hid_dim + emb_dim, output_dim)\n","        self.fc_out = nn.Linear(hid_dim + hid_dim + emb_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def forward(self, input, hidden, cell, encoder_outputs):\n","        input = input.unsqueeze(1)  # [batch_size, 1]\n","        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, emb_dim]\n","\n","        a = self.attention(hidden[-1], encoder_outputs).unsqueeze(1)  # [batch_size, 1, src_len]\n","        weighted = torch.bmm(a, encoder_outputs)  # [batch_size, 1, hid_dim]\n","\n","        rnn_input = torch.cat((embedded, weighted), dim=2)  # [batch_size, 1, emb_dim + hid_dim]\n","\n","        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))  # output: [batch_size, 1, hid_dim]\n","\n","        embedded = embedded.squeeze(1)  # [batch_size, emb_dim]\n","        output = output.squeeze(1)  # [batch_size, hid_dim]\n","        weighted = weighted.squeeze(1)  # [batch_size, hid_dim]\n","\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # [batch_size, output_dim]\n","\n","        return prediction, hidden, cell"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1715997031830,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"Vk_DrL5i1hdD"},"outputs":[],"source":["# Seq2Seq with Attention\n","class Seq2SeqWithAttention(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=TEACHING_FORCING_RATIO):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","        encoder_outputs, hidden, cell = self.encoder(src)\n","        input = trg[:, 0]\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n","            outputs[:, t, :] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Extension"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1 Extension 1: Preprocessing data <num> and frequency"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Clean num\n","def clean_num(df, columns):\n","    for col in columns:\n","        df[col] = df[col].apply(lambda x: re.sub(r'\\d+', '<num>', x)) \n","        # df[col] = df[col].apply(lambda x: re.sub(r'\\b\\d+\\b', '<num>', x))\n","    return df\n","\n","# Clean data\n","columns_ex = ['Ingredients', 'Recipe']\n","train_data_ex = clean_num(train_data_sampling, columns)\n","dev_data_ex = clean_num(dev_data, columns)\n","test_data_ex = clean_num(test_data, columns)\n","\n","# Word2idx Vocab with >= 2 times vocab and att <num>\n","def build_vocab_ex(data, min_freq=2):\n","    word_freq = {}\n","    for text in pd.concat([data['Ingredients'], data['Recipe']]):\n","        for word in text.split():\n","            if word in word_freq:\n","                word_freq[word] += 1\n","            else:\n","                word_freq[word] = 1\n","\n","    # Prune vocabulary based on frequency\n","    word2idx = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3, \"<num>\": 4}\n","    idx = 5\n","    for word, freq in word_freq.items():\n","        if freq >= min_freq:\n","            word2idx[word] = idx\n","            idx += 1\n","\n","    # Add padding token to match the expected size\n","    word2idx[\"<pad_extra>\"] = len(word2idx)\n","\n","    return word2idx\n","\n","vocab_ex  = build_vocab_ex(train_data_ex)\n","\n","# Inverse dictionary\n","idx2word_ex = {idx: word for word, idx in vocab_ex.items()}\n","\n","print(\"Vocab-ex size:\", len(vocab_ex))\n","print({k: vocab_ex[k] for k in list(vocab_ex)[:20]})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create dataset and loader\n","train_dataset_ex = RecipeDataset(train_data_ex['Ingredients'].tolist(), train_data_ex['Recipe'].tolist(), vocab_ex)\n","dev_dataset_ex = RecipeDataset(dev_data_ex['Ingredients'].tolist(), dev_data_ex['Recipe'].tolist(), vocab_ex)\n","test_dataset_ex = RecipeDataset(test_data_ex['Ingredients'].tolist(), test_data_ex['Recipe'].tolist(), vocab_ex)\n","\n","train_loader_ex = torch.utils.data.DataLoader(train_dataset_ex, batch_size=BATCH_SIZE, shuffle=True)\n","dev_loader_ex = torch.utils.data.DataLoader(dev_dataset_ex, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader_ex = torch.utils.data.DataLoader(test_dataset_ex, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2. Extension 2: Using Pretrained Embeddings GLoVe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EMBED_DIM_GLOVE = 50\n","\n","def load_embeddings(vocab,  embedding_dim, glove_data):\n","    embeddings_index = {}\n","    with open(glove_data, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","\n","    vocab_size = len(vocab) # Adjust the size of the embedding matrix\n","    print(f\"Vocabulary size: {vocab_size}\")\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","    \n","    for word, i in vocab.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            if i < vocab_size:  # Ensure index is within bounds\n","                embedding_matrix[i] = embedding_vector\n","            else:\n","                print(f\"Index {i} for word {word} is out of bounds with vocab size {vocab_size}\")\n","\n","    embedding_matrix[-1] = np.zeros(embedding_dim) ##\n","\n","    return torch.tensor(embedding_matrix, dtype=torch.float32)\n","\n","pretrained_embeddings = load_embeddings(vocab_ex, EMBED_DIM_GLOVE, GLOVE_PATH)\n","print(f\"Pretrained embedding matrix shape: {pretrained_embeddings.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# encoder adn decoder for extension\n","class EncoderWithEmbeddings(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, dropout, embeddings):\n","        super().__init__()\n","        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=False)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=NUM_LAYER, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        embedded = self.dropout(self.embedding(src))\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        return outputs, hidden, cell\n","\n","class DecoderWithAttentionAndEmbeddings(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, attention, dropout, embeddings):\n","        super().__init__()\n","        self.output_dim = output_dim\n","        self.attention = attention\n","        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=False)\n","        self.rnn = nn.LSTM(hid_dim + emb_dim, hid_dim, num_layers=NUM_LAYER, batch_first=True)\n","        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, cell, encoder_outputs):\n","        input = input.unsqueeze(1)\n","        embedded = self.dropout(self.embedding(input))\n","        a = self.attention(hidden[-1], encoder_outputs).unsqueeze(1)\n","        weighted = torch.bmm(a, encoder_outputs)\n","        rnn_input = torch.cat((embedded, weighted), dim=2)\n","        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n","        embedded = embedded.squeeze(1)\n","        output = output.squeeze(1)\n","        weighted = weighted.squeeze(1)\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n","        return prediction, hidden, cell"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Seq2Seq with Attention and Pretrained Embeddings\n","class Seq2SeqWithAttentionAndEmbeddings(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=TEACHING_FORCING_RATIO):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","        encoder_outputs, hidden, cell = self.encoder(src)\n","        input = trg[:, 0]\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n","            outputs[:, t, :] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Seq2SeqWithContentPlanning(nn.Module):\n","    def __init__(self, encoder, content_planner, sequence_generator, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.content_planner = content_planner\n","        self.sequence_generator = sequence_generator\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=TEACHING_FORCING_RATIO):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.sequence_generator.output_dim\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","\n","        # Encode the input sequence\n","        encoder_outputs, hidden, cell = self.encoder(src) \n","        encoder_outputs = encoder_outputs.mean(dim=1)\n","        encoder_outputs = encoder_outputs.long()\n","        content_plan = self.content_planner(encoder_outputs)\n","\n","        # Initialize the hidden and cell states of the sequence generator\n","        input = trg[:, 0]\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.sequence_generator(input, hidden, cell, content_plan)\n","            outputs[:, t, :] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs\n"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Train & Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1 train & evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715993764056,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"Xh9I-UHdFg-9"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","    model.train()\n","    epoch_loss = 0\n","\n","    for i, batch in enumerate(iterator):\n","        src, trg = batch\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(src, trg)\n","        output_dim = output.shape[-1]\n","\n","        output = output[:, 1:].reshape(-1, output_dim)\n","        trg = trg[:, 1:].reshape(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            src, trg = batch\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0)  # Turn off teacher forcing\n","            output_dim = output.shape[-1]\n","\n","            output = output[:, 1:].reshape(-1, output_dim)\n","            trg = trg[:, 1:].reshape(-1)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{},"source":["## 4.2 Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3113,"status":"ok","timestamp":1715993770604,"user":{"displayName":"爵太郎","userId":"07896393839120566142"},"user_tz":-600},"id":"N9sIFFiqF18a"},"outputs":[],"source":["# Model & Training Configurations\n","INPUT_DIM = len(vocab)\n","OUTPUT_DIM = len(vocab)\n","INPUT_DIM_EX = len(vocab_ex)\n","OUTPUT_DIM_EX = len(vocab_ex) \n","EMB_DIM = 256\n","HID_DIM = 256\n","DROPOUT = 0.1\n","LEARNING_RATE = 0.001\n","NUM_STAGES = 5 \n","PATIENCE = 5\n","N_EPOCHS = 10\n","CLIP = 1\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Model 1: Seq2Seq\n","encoder1 = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n","decoder1 = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n","model1 = Seq2Seq(encoder1, decoder1, device).to(device)\n","\n","optimizer1 = optim.Adam(model1.parameters(), lr=LEARNING_RATE)\n","criterion1 = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n","\n","# Model 2: Seq2Seq with Attention\n","attn2 = Attention(HID_DIM)\n","encoder2 = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n","decoder2 = DecoderWithAttention(OUTPUT_DIM, EMB_DIM, HID_DIM, attn2, DROPOUT)\n","model2 = Seq2SeqWithAttention(encoder2, decoder2, device).to(device)\n","\n","optimizer2 = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n","criterion2 = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n","\n","# Model 3: Processed Data with Attention\n","attn3 = Attention(HID_DIM)\n","encoder3 = Encoder(INPUT_DIM_EX, EMB_DIM, HID_DIM, DROPOUT)\n","decoder3 = DecoderWithAttention(OUTPUT_DIM_EX, EMB_DIM, HID_DIM, attn3, DROPOUT)\n","model3 = Seq2SeqWithAttention(encoder3, decoder3, device).to(device)\n","\n","optimizer3 = optim.Adam(model3.parameters(), lr=LEARNING_RATE)\n","criterion3 = nn.CrossEntropyLoss(ignore_index=vocab_ex['<pad>'])\n","\n","# Model 4: Embedding and Processed Data with Attention\n","attn4 = Attention(HID_DIM)\n","encoder4 = EncoderWithEmbeddings(INPUT_DIM_EX, EMBED_DIM_GLOVE, HID_DIM, DROPOUT, pretrained_embeddings)\n","decoder4 = DecoderWithAttentionAndEmbeddings(OUTPUT_DIM_EX, EMBED_DIM_GLOVE, HID_DIM, attn4, DROPOUT, pretrained_embeddings)\n","model4 = Seq2SeqWithAttentionAndEmbeddings(encoder4, decoder4, device).to(device)\n","\n","optimizer4 = optim.Adam(model4.parameters(), lr=LEARNING_RATE)\n","criterion4 = nn.CrossEntropyLoss(ignore_index=vocab_ex['<pad>'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=PATIENCE, verbose=False, delta=0.01):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = float('inf')\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model):\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            if self.verbose:\n","                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), 'checkpoint.pt')\n","        self.val_loss_min = val_loss\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","models = [model1, model2, model3, model4, model5]\n","train_losses = [[] for _ in range(len(models))]\n","valid_losses = [[] for _ in range(len(models))]\n","training_times = [0 for _ in range(len(models))]  # Initialize with zeros\n","\n","# Early stopping initialization\n","early_stoppings = [EarlyStopping(patience=PATIENCE, verbose=True) for _ in range(len(models))]\n","\n","\n","# Training each model with early stopping and timing\n","for i, model in enumerate(models):\n","# start_model_idx = 3  # Start from model 3 (index 2)\n","# for i in range(start_model_idx, len(models)):\n","#     model = models[i] ########\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'] if i < 2 else vocab_ex['<pad>'])\n","    train_loader_current = train_loader if i < 2 else train_loader_ex\n","    dev_loader_current = dev_loader if i < 2 else dev_loader_ex\n","\n","    # Initialize total training time for the current model\n","    training_time = 0\n","\n","    for epoch in range(N_EPOCHS):\n","        start_time = time.time()\n","\n","        train_loss = train(model, train_loader_current, optimizer, criterion, CLIP)\n","        valid_loss = evaluate(model, dev_loader_current, criterion)\n","\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","        # Accumulate total training time\n","        training_time += (end_time - start_time)\n","\n","        train_losses[i].append(train_loss)\n","        valid_losses[i].append(valid_loss)\n","\n","        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","\n","        early_stoppings[i](valid_loss, model)\n","\n","        if early_stoppings[i].early_stop:\n","            print(\"Early stopping\")\n","            break\n","\n","    # Save the final model\n","    torch.save(model.state_dict(), f'model{i+1}.pth')\n","\n","    # Store the training time for the current model\n","    training_times.append(training_time)\n","    total_mins, total_secs = epoch_time(0, training_time)\n","    print(f'Total training time for model {i+1}: {total_mins}m {total_secs}s')\n","\n","    # Save the losses and training times to files after each model is trained\n","    with open(f'train_losses_model{i+1}.pkl', 'wb') as f:\n","        pickle.dump(train_losses[i], f)\n","\n","    with open(f'valid_losses_model{i+1}.pkl', 'wb') as f:\n","        pickle.dump(valid_losses[i], f)\n","\n","    with open(f'training_time_model{i+1}.pkl', 'wb') as f:\n","        pickle.dump(training_times[i], f)"]},{"cell_type":"markdown","metadata":{},"source":["# 4.5 loading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import pickle\n","\n","# model1 = Seq2Seq(encoder1, decoder1, device).to(device)\n","# model2 = Seq2SeqWithAttention(encoder2, decoder2, device).to(device)\n","# model3 = Seq2SeqWithAttention(encoder3, decoder3, device).to(device)\n","# model4 = Seq2SeqWithAttentionAndEmbeddings(encoder4, decoder4, device).to(device)\n","\n","# model1.load_state_dict(torch.load('model1.pth', map_location=torch.device('cpu')))\n","# model2.load_state_dict(torch.load('model2.pth', map_location=torch.device('cpu')))\n","# model3.load_state_dict(torch.load('model3.pth', map_location=torch.device('cpu')))\n","# model4.load_state_dict(torch.load('model4.pth', map_location=torch.device('cpu')))\n","\n","# #List of models\n","# models = [model1, model2, model3, model4, model5]\n","\n","# # Reading train_losses\n","# train_losses = []\n","# for i in range(len(models)):\n","#     with open(f'train_losses_model{i+1}.pkl', 'rb') as f:\n","#         train_losses.append(pickle.load(f))\n","\n","# # Reading valid_losses\n","# valid_losses = []\n","# for i in range(len(models)):\n","#     with open(f'valid_losses_model{i+1}.pkl', 'rb') as f:\n","#         valid_losses.append(pickle.load(f))\n","\n","# # Reading training_times\n","# training_times = []\n","# for i in range(len(models)):\n","#     with open(f'training_time_model{i+1}.pkl', 'rb') as f:\n","#         training_times.append(pickle.load(f))"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Analyze"]},{"cell_type":"markdown","metadata":{},"source":["## 5.1 Statistic"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute statistics\n","def compute_statistics(data, vocab, special_tokens):\n","    num_samples = len(data)\n","    vocab_size = len(vocab) - len(special_tokens)\n","    \n","    ingredients_lengths = data['Ingredients'].apply(lambda x: len([word for word in x.split() if word not in special_tokens]))\n","    recipes_lengths = data['Recipe'].apply(lambda x: len([word for word in x.split() if word not in special_tokens]))\n","    \n","    ingredients_stats = {\n","        'num_samples': num_samples,\n","        'vocab_size': vocab_size,\n","        'min_length': ingredients_lengths.min(),\n","        'max_length': ingredients_lengths.max(),\n","        'avg_length': ingredients_lengths.mean()\n","    }\n","    \n","    recipes_stats = {\n","        'num_samples': num_samples,\n","        'vocab_size': vocab_size,\n","        'min_length': recipes_lengths.min(),\n","        'max_length': recipes_lengths.max(),\n","        'avg_length': recipes_lengths.mean()\n","    }\n","    \n","    return ingredients_stats, recipes_stats\n","\n","# Special tokens\n","special_tokens_standard = {'<pad>', '<sos>', '<eos>', '<unk>'}\n","special_tokens_extended = {'<pad>', '<sos>', '<eos>', '<unk>', '<num>'}\n","\n","# Compute statistics for both datasets\n","train_stats_ing, train_stats_rec = compute_statistics(train_data, vocab, special_tokens_standard)\n","dev_stats_ing, dev_stats_rec = compute_statistics(dev_data, vocab, special_tokens_standard)\n","test_stats_ing, test_stats_rec = compute_statistics(test_data, vocab, special_tokens_standard)\n","\n","train_stats_ing_ex, train_stats_rec_ex = compute_statistics(train_data_ex, vocab_ex, special_tokens_extended)\n","dev_stats_ing_ex, dev_stats_rec_ex = compute_statistics(dev_data_ex, vocab_ex, special_tokens_extended)\n","test_stats_ing_ex, test_stats_rec_ex = compute_statistics(test_data_ex, vocab_ex, special_tokens_extended)\n","\n","# Function to prepare statistics for table\n","def statistics_table(stats, dataset_name):\n","    table_data = []\n","    for category in ['Ingredients', 'Recipes']:\n","        category_stats = stats[category]\n","        table_data.append([\n","            dataset_name, \n","            category, \n","            category_stats['num_samples'], \n","            category_stats['vocab_size'], \n","            category_stats['min_length'], \n","            category_stats['max_length'], \n","            category_stats['avg_length']\n","        ])\n","    return table_data\n","\n","# Gather all statistics in a list of lists\n","all_stats = []\n","all_stats.extend(statistics_table({'Ingredients': train_stats_ing, 'Recipes': train_stats_rec}, 'Training Data'))\n","all_stats.extend(statistics_table({'Ingredients': dev_stats_ing, 'Recipes': dev_stats_rec}, 'Dev Data'))\n","all_stats.extend(statistics_table({'Ingredients': test_stats_ing, 'Recipes': test_stats_rec}, 'Test Data'))\n","all_stats.extend(statistics_table({'Ingredients': train_stats_ing_ex, 'Recipes': train_stats_rec_ex}, 'Training Data (Ex)'))\n","all_stats.extend(statistics_table({'Ingredients': dev_stats_ing_ex, 'Recipes': dev_stats_rec_ex}, 'Dev Data (Ex)'))\n","all_stats.extend(statistics_table({'Ingredients': test_stats_ing_ex, 'Recipes': test_stats_rec_ex}, 'Test Data (Ex)'))\n","\n","# Create a DataFrame for pretty printing\n","columns = ['Dataset', 'Category', 'Num Samples', 'Vocab Size', 'Min Length', 'Max Length', 'Avg Length']\n","stats_df = pd.DataFrame(all_stats, columns=columns)\n","\n","print(stats_df.to_string(index=False))"]},{"cell_type":"markdown","metadata":{},"source":["## 5.2 Diagram Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","model_names = ['Baseline 1', 'Baseline 2', 'Extension 1', 'Extension 2']\n","colors = ['blue', 'red', 'green', 'purple']\n","linestyles = ['solid', 'dashed']\n","\n","plt.figure(figsize=(10, 6))\n","\n","for i, (model, color) in enumerate(zip(model_names, colors)):\n","    plt.plot(train_losses[i], color=color, linestyle=linestyles[0], label=f'{model} - Training')\n","    plt.plot(valid_losses[i], color=color, linestyle=linestyles[1], label=f'{model} - Validation')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 5.3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.meteor_score import meteor_score\n","\n","def avg_percentage_given_items(ingredients, generated):\n","    given_items = set(ingredients)\n","    gen_items = set(generated)\n","    common_items = given_items.intersection(gen_items)\n","    return len(common_items) / len(given_items) * 100\n","\n","def avg_extra_items(ingredients, generated):\n","    given_items = set(ingredients)\n","    gen_items = set(generated)\n","    extra_items = gen_items.difference(given_items)\n","    return len(extra_items)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_metrics(reference, candidate):\n","    bleu_4 = sentence_bleu([reference], candidate, weights=(0.25, 0.25, 0.25, 0.25))\n","    meteor = meteor_score([reference], candidate)\n","\n","    reference_ingredients = [word[1:-1] for word in reference if word.startswith('<') and word.endswith('>')]\n","    candidate_ingredients = [word[1:-1] for word in candidate if word.startswith('<') and word.endswith('>')]\n","    \n","    common_ingredients = set(reference_ingredients).intersection(set(candidate_ingredients))\n","    avg_given = len(common_ingredients) / len(reference_ingredients) if reference_ingredients else 0\n","    avg_extra = len(candidate_ingredients) - len(common_ingredients)\n","    \n","    return bleu_4, meteor, avg_given, avg_extra\n","\n","test_results = []\n","\n","for i, model in enumerate(models):\n","    model.eval()\n","    references = []\n","    candidates = []\n","\n","    with torch.no_grad():\n","        test_loader_current = test_loader if i < 2 else test_loader_ex\n","        idx2word_current = idx2word if i < 2 else idx2word_ex\n","        vocab_current = vocab if i < 2 else vocab_ex\n","\n","        for batch in test_loader_current:\n","            src, trg = batch[0].to(device), batch[1].to(device)\n","            output = model(src, trg, 0)  # Turn off teacher forcing\n","            output = output.argmax(dim=-1)\n","\n","            for j in range(trg.size(0)):\n","                ref = trg[j].cpu().numpy()\n","                cand = output[j].cpu().numpy()\n","\n","                ref_text = [idx2word_current[idx] for idx in ref if idx not in {vocab_current['<pad>'], vocab_current['<sos>'], vocab_current['<eos>'], vocab_current.get('<num>', -1)}]\n","                cand_text = [idx2word_current[idx] for idx in cand if idx not in {vocab_current['<pad>'], vocab_current['<sos>'], vocab_current['<eos>'], vocab_current.get('<num>', -1)}]\n","\n","                references.append(ref_text)\n","                candidates.append(cand_text)\n","\n","    bleu_4_scores = []\n","    meteor_scores = []\n","    avg_given_scores = []\n","    avg_extra_scores = []\n","\n","    for ref, cand in zip(references, candidates):\n","        bleu_4, meteor, avg_given, avg_extra = calculate_metrics(ref, cand)\n","        bleu_4_scores.append(bleu_4)\n","        meteor_scores.append(meteor)\n","        avg_given_scores.append(avg_given)\n","        avg_extra_scores.append(avg_extra)\n","\n","    test_results.append({\n","        \"BLEU-4\": np.mean(bleu_4_scores),\n","        \"METEOR\": np.mean(meteor_scores),\n","        \"Avg. % given items\": np.mean(avg_given_scores),\n","        \"Avg. extra items\": np.mean(avg_extra_scores)\n","    })\n","\n","# Convert to DataFrame for easier presentation\n","results_df = pd.DataFrame(test_results, index=[\"Baseline 1\", \"Baseline 2\", \"Extension 1\", \"Extension 2\", \"Content Plan\"])\n","results_df = results_df.round(10)\n","print(results_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# gold sample\n","ingredients = \"2 c sugar, 1/4 c lemon juice, 1 c water, 1/3 c orange juice, 8 c strawberries\"\n","gold_recipe = \"combine <sugar> and <water> in medium saucepan . Heat , stirring , until <sugar> dissolves , then boil 5 minutes . cool . force <strawberries> through food mill or blend in blender or food processor .  strain to remove seeds , if desired . blend the puree and <lemon juice> and <orange juice> into syrup .  pour into freezer trays and freeze . remove from freezer 20 minutes before serving . turn into bowl and stir until smooth .\"\n","generated_recipe = \"Combine <sugar> and <water> in a medium saucepan . Heat, stirring, until <sugar> dissolves . Bring to a boil and let simmer for 5 minutes . Remove from heat and allow to cool . In a blender or food processor , combine <strawberries> and <cantaloupe> . Blend until smooth . Strain the mixture to remove any seeds and fibers, if desired. Stir the puree into the cooled syrup along with the <lemon juice> and <orange juice> . Pour the mixture into a large bowl and gently fold in the <vanilla ice cream> until well mixed . Freeze in a container for at least 4 hours . Before serving , let it sit at room temperature for 20 minutes to soften . Stir well to achieve a smooth consistency and serve chilled .\"\n","\n","# Tokenize the gold and generated recipes\n","gold_recipe_tokens = gold_recipe.split()\n","generated_recipe_tokens = generated_recipe.split()\n","\n","# Calculate the metrics\n","sample_results = calculate_metrics(gold_recipe_tokens, generated_recipe_tokens)\n","\n","# Create a DataFrame for the sample results\n","sample_results_df = pd.DataFrame([sample_results], columns=[\"BLEU-4\", \"METEOR\", \"Avg. % given items\", \"Avg. extra items\"], index=[\"Sample\"])\n","sample_results_df = sample_results_df.round(10)\n","print(sample_results_df)"]},{"cell_type":"markdown","metadata":{},"source":["## 5.4 recipe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_recipe(model, ingredient, vocab, idx2word, max_length=MAX_LENGTH):\n","    model.eval()\n","    encoded_ingredient = [vocab.get(word, vocab['<unk>']) for word in ingredient.split()] + [vocab['<eos>']]\n","    ingredient_tensor = torch.tensor([encoded_ingredient], dtype=torch.long).to(device)\n","\n","    with torch.no_grad():\n","        # General Seq2Seq model\n","        output = model(ingredient_tensor, torch.zeros_like(ingredient_tensor).to(device), 0)  # No teacher forcing\n","        output = output.squeeze(0).argmax(dim=1)\n","        recipe = ' '.join([idx2word[idx.item()] for idx in output if idx.item() != vocab['<pad>'] and idx.item() != vocab['<eos>']])\n","        return recipe\n","\n","# Sample ingredient list\n","ingredients_sample = \"2 c sugar, 1/4 c lemon juice, 1 c water, 1/3 c orange juice, 8 c strawberries\"\n","\n","# Generate recipes using all models\n","generated_recipes = []\n","\n","for i, model in enumerate(models):\n","    if i < 2:\n","        recipe = generate_recipe(model, ingredients_sample, vocab, idx2word)\n","    else:\n","        recipe = generate_recipe(model, ingredients_sample, vocab_ex, idx2word_ex)\n","    generated_recipes.append(recipe)\n","\n","# Create a DataFrame to store the results\n","qualitative_results = pd.DataFrame({\n","    \"Model\": [\"Baseline 1\", \"Baseline 2\", \"Extension 1\", \"Extension 2\", \"Content Plan\"],\n","    \"Generated Recipe\": generated_recipes\n","})\n","\n","\n","print(\"Qualitative Samples from all models on the given ingredient list:\")\n","print(qualitative_results)\n","\n","# Save the results to a CSV file\n","qualitative_results.to_csv(\"generated_recipe.csv\", index=False)\n","\n","# Discuss the results\n","for i, row in qualitative_results.iterrows():\n","    print(f\"\\nModel: {row['Model']}\")\n","    print(f\"Generated Recipe: {row['Generated Recipe']}\")\n","\n","print(\"Qualitative Samples from all models on the given ingredient list:\")\n","print(qualitative_results)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOqRWWw1r0GVwI1tONPZMbL","gpuType":"T4","provenance":[{"file_id":"1bBL2u0e1ZZ-gR-3P_qJii7hAyP8eBOOh","timestamp":1715992600355}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
